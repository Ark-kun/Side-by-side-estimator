{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 610,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math as math\n",
    "import cntk as c\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "c.cntk_py.set_fixed_random_seed(1)\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cntk_erf(x):\n",
    "    return c.tanh((2 / math.sqrt(math.pi)) * x)\n",
    "\n",
    "#x ~ Norm(Mean, Var)\n",
    "#Prob(x<b) = 0.5* (1 + erf((b-Mean)/(sqrt(2*Var))))\n",
    "#Prob(a<x<b) = 0.5 * erf((b-Mean)/(sqrt(2*Var)))] - 0.5 * erf((a-Mean)/(sqrt(2*Var)))]\n",
    "def cntk_normal_cdf(x, mean, variance):\n",
    "    #return 0.5 * (1 + cntk_erf((x - mean) / c.sqrt(2 * variance))) #Neede to reverse the order of subtraction in x - mean so that the resulting shape (after broadcasting) is (mean.shape, x.shape)\n",
    "    #return 0.5 * (1 + cntk_erf(-(mean - x) / c.sqrt(2 * variance)))    \n",
    "    return 0.5 * (1 - cntk_erf((mean - x) / c.sqrt(2 * variance))) #same as above with 1 less negation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dtype=np.float64\n",
    "numberOfQueries = 10\n",
    "numberOfIntervals = 3\n",
    "numberOfBoundaries = numberOfIntervals - 1\n",
    "deltaMeans = c.parameter(shape=(numberOfQueries, 1), init=c.normal(1), dtype=dtype, name='deltaMeans')\n",
    "deltaStDevs = c.parameter(shape=(numberOfQueries, 1), init=3, dtype=dtype, name='deltaStDevs')\n",
    "#deltaStDevs = c.constant(shape=(numberOfQueries, 1), value=3, dtype=dtype, name='deltaStDevs')\n",
    "deltaVariances = c.square(deltaStDevs)\n",
    "\n",
    "initialInnerBoundaries = np.linspace(-1, 1, numberOfBoundaries, dtype=dtype)\n",
    "\n",
    "#innerBoundaries = c.parameter(numberOfBoundaries, init=[-1, 1])\n",
    "#innerBoundaries = c.parameter(numberOfBoundaries, init=(np.arange(numberOfBoundaries) / (numberOfBoundaries - 1) * 2 - 1)) #equally spaced boundaries in [-1, 1] range\n",
    "innerBoundaries = c.parameter(numberOfBoundaries, init=initialInnerBoundaries, dtype=dtype, name='innerBoundaries')\n",
    "#leftmostBoundary = c.constant(-math.inf, dtype=dtype, name='leftmostBoundary')\n",
    "#rightmostBoundary = c.constant(+math.inf, dtype=dtype, name='rightmostBoundary')\n",
    "#boundaries = c.splice(leftmostBoundary, innerBoundaries, rightmostBoundary, name='boundaries')\n",
    "\n",
    "#vvv WTF? CNTK could not optimize the loss even though the input was not a constant but a parameter\n",
    "#countsForIntervals = c.parameter(shape=(numberOfQueries, numberOfIntervals), init=np.random.randint(13, size=(numberOfQueries, numberOfIntervals)), dtype=np.int) #input\n",
    "realDeltaMeans = np.random.normal(0, 3, size=(numberOfQueries))\n",
    "countsForIntervalsInit = np.asarray([np.histogram(np.random.normal(mean, 1, size=50), bins=np.concatenate(([-np.inf], initialInnerBoundaries, [np.inf])))[0] for mean in realDeltaMeans])\n",
    "\n",
    "countsForIntervals = c.constant(shape=(numberOfQueries, numberOfIntervals), value=countsForIntervalsInit, name='countsForIntervals') #input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#cntk_erf(boundaries).eval()\n",
    "\n",
    "#cumulativeProbabilitiesForBoundaries = cntk_normal_cdf(boundaries, deltaMeans, deltaVariances) #Has NaN variance derivatives at infinities, so we add the infinite boundaries later\n",
    "cumulativeProbabilitiesForInnerBoundaries = cntk_normal_cdf(innerBoundaries, deltaMeans, deltaVariances)\n",
    "cumulativeProbabilitiesForBoundaries = c.splice(0, cumulativeProbabilitiesForInnerBoundaries, 1)\n",
    "\n",
    "intervalProbabilities = c.slice(cumulativeProbabilitiesForBoundaries, 1, 1, 0) - c.slice(cumulativeProbabilitiesForBoundaries, 1, 0, -1) #P[i] = CDF[i]-CDF[i-1]\n",
    "\n",
    "#intervalProbabilities.eval()\n",
    "\n",
    "#(c.pow(intervalProbabilities, [[2,2,2],[1,1,1],[1,1,1],[1,1,1],[1,1,1],[1,1,1],[1,1,1],[1,1,1],[1,1,1],[1,1,1]]  )).eval() #works!\n",
    "\n",
    "occurrenceProbablilities = c.pow(intervalProbabilities, countsForIntervals, name='occurrenceProbablilities') #element_pow\n",
    "\n",
    "totalProbability = c.reduce_prod(occurrenceProbablilities)\n",
    "logTotalProbability = c.reduce_sum(c.log(occurrenceProbablilities))\n",
    "\n",
    "#print((0 + countsForIntervals).eval())\n",
    "#print(occurrenceProbablilities.eval())\n",
    "#totalProbability.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 614,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Parameter('deltaStDevs', [], [10 x 1]): array([[  7.15839658],\n",
      "       [  0.77102241],\n",
      "       [ -3.21799788],\n",
      "       [  4.9576091 ],\n",
      "       [  9.99648852],\n",
      "       [ 20.06620402],\n",
      "       [  9.59364466],\n",
      "       [  5.28186027],\n",
      "       [  8.05820313],\n",
      "       [ -5.63429198]]), Parameter('deltaMeans', [], [10 x 1]): array([[ 17.49218012],\n",
      "       [-13.06928597],\n",
      "       [ -9.05971922],\n",
      "       [-16.31672941],\n",
      "       [ 18.47206956],\n",
      "       [-21.65831963],\n",
      "       [ 18.5051491 ],\n",
      "       [-16.1920497 ],\n",
      "       [ 16.71266962],\n",
      "       [  1.70127679]]), Parameter('innerBoundaries', [], [2]): array([ 37.81882732, -34.40606858])}\n"
     ]
    }
   ],
   "source": [
    "logTotalProbability.eval()\n",
    "\n",
    "logTotalProbability.forward(None)[1]\n",
    "\n",
    "df, fv = logTotalProbability.forward({}, [logTotalProbability.output], set([logTotalProbability.output]))\n",
    "#print(df, fv)\n",
    "value = list(fv.values())[0]\n",
    "#print(value)\n",
    "grad = logTotalProbability.backward(df, {logTotalProbability.output: np.ones_like(value)}, logTotalProbability.parameters)\n",
    "\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 615,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate per minibatch: 0.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\local\\Anaconda3-4.1.1-Windows-x86_64\\envs\\cntk-py35\\lib\\site-packages\\cntk\\core.py:351: UserWarning: your data is of type \"float32\", but your input variable (uid \"Input14341\") expects \"<class 'numpy.float64'>\". Please convert your data beforehand to speed up training.\n",
      "  (sample.dtype, var.uid, str(var.dtype)))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Minibatch[   1- 100]: loss = 210.401997 * 100, metric = 107.04% * 100;\n",
      " Minibatch[ 101- 200]: loss = 164.642073 * 100, metric = 88.71% * 100;\n",
      " Minibatch[ 201- 300]: loss = 164.379703 * 100, metric = 93.11% * 100;\n",
      " Minibatch[ 301- 400]: loss = 164.256477 * 100, metric = 96.09% * 100;\n",
      " Minibatch[ 401- 500]: loss = 164.188368 * 100, metric = 98.39% * 100;\n",
      " Minibatch[ 501- 600]: loss = 164.143764 * 100, metric = 100.28% * 100;\n",
      " Minibatch[ 601- 700]: loss = 164.111920 * 100, metric = 101.89% * 100;\n",
      " Minibatch[ 701- 800]: loss = 164.087800 * 100, metric = 103.30% * 100;\n",
      " Minibatch[ 801- 900]: loss = 164.068824 * 100, metric = 104.55% * 100;\n",
      " Minibatch[ 901-1000]: loss = 164.053465 * 100, metric = 105.67% * 100;\n",
      " Minibatch[1001-1100]: loss = 164.040750 * 100, metric = 106.70% * 100;\n",
      " Minibatch[1101-1200]: loss = 164.030026 * 100, metric = 107.64% * 100;\n",
      " Minibatch[1201-1300]: loss = 164.020850 * 100, metric = 108.51% * 100;\n",
      " Minibatch[1301-1400]: loss = 164.012901 * 100, metric = 109.33% * 100;\n",
      " Minibatch[1401-1500]: loss = 164.005941 * 100, metric = 110.08% * 100;\n",
      " Minibatch[1501-1600]: loss = 163.999792 * 100, metric = 110.80% * 100;\n",
      " Minibatch[1601-1700]: loss = 163.994317 * 100, metric = 111.47% * 100;\n",
      " Minibatch[1701-1800]: loss = 163.989407 * 100, metric = 112.11% * 100;\n",
      " Minibatch[1801-1900]: loss = 163.984977 * 100, metric = 112.71% * 100;\n",
      " Minibatch[1901-2000]: loss = 163.980959 * 100, metric = 113.29% * 100;\n",
      " Minibatch[2001-2100]: loss = 163.977296 * 100, metric = 113.84% * 100;\n",
      " Minibatch[2101-2200]: loss = 163.973943 * 100, metric = 114.37% * 100;\n",
      " Minibatch[2201-2300]: loss = 163.970860 * 100, metric = 114.87% * 100;\n",
      " Minibatch[2301-2400]: loss = 163.968016 * 100, metric = 115.35% * 100;\n",
      " Minibatch[2401-2500]: loss = 163.965383 * 100, metric = 115.82% * 100;\n",
      " Minibatch[2501-2600]: loss = 163.962939 * 100, metric = 116.27% * 100;\n",
      " Minibatch[2601-2700]: loss = 163.960664 * 100, metric = 116.70% * 100;\n",
      " Minibatch[2701-2800]: loss = 163.958542 * 100, metric = 117.11% * 100;\n",
      " Minibatch[2801-2900]: loss = 163.956557 * 100, metric = 117.52% * 100;\n",
      " Minibatch[2901-3000]: loss = 163.954698 * 100, metric = 117.91% * 100;\n",
      " Minibatch[3001-3100]: loss = 163.952956 * 100, metric = 118.29% * 100;\n",
      " Minibatch[3101-3200]: loss = 163.951322 * 100, metric = 118.65% * 100;\n",
      " Minibatch[3201-3300]: loss = 163.949795 * 100, metric = 119.01% * 100;\n",
      " Minibatch[3301-3400]: loss = 163.948377 * 100, metric = 119.36% * 100;\n",
      " Minibatch[3401-3500]: loss = 163.947094 * 100, metric = 119.70% * 100;\n",
      " Minibatch[3501-3600]: loss = 163.946025 * 100, metric = 120.03% * 100;\n",
      " Minibatch[3601-3700]: loss = 163.945407 * 100, metric = 120.36% * 100;\n",
      " Minibatch[3701-3800]: loss = 163.945391 * 100, metric = 120.70% * 100;\n",
      " Minibatch[3801-3900]: loss = 163.944825 * 100, metric = 121.05% * 100;\n",
      " Minibatch[3901-4000]: loss = 163.943717 * 100, metric = 121.40% * 100;\n",
      " Minibatch[4001-4100]: loss = 163.942760 * 100, metric = 121.73% * 100;\n",
      " Minibatch[4101-4200]: loss = 163.941851 * 100, metric = 122.06% * 100;\n",
      " Minibatch[4201-4300]: loss = 163.940966 * 100, metric = 122.38% * 100;\n",
      " Minibatch[4301-4400]: loss = 163.940114 * 100, metric = 122.70% * 100;\n",
      " Minibatch[4401-4500]: loss = 163.939293 * 100, metric = 123.01% * 100;\n",
      " Minibatch[4501-4600]: loss = 163.938501 * 100, metric = 123.32% * 100;\n",
      " Minibatch[4601-4700]: loss = 163.937737 * 100, metric = 123.62% * 100;\n",
      " Minibatch[4701-4800]: loss = 163.936998 * 100, metric = 123.91% * 100;\n",
      " Minibatch[4801-4900]: loss = 163.936285 * 100, metric = 124.20% * 100;\n",
      " Minibatch[4901-5000]: loss = 163.935595 * 100, metric = 124.48% * 100;\n",
      " Minibatch[5001-5100]: loss = 163.934927 * 100, metric = 124.76% * 100;\n",
      " Minibatch[5101-5200]: loss = 163.934281 * 100, metric = 125.04% * 100;\n",
      " Minibatch[5201-5300]: loss = 163.933654 * 100, metric = 125.31% * 100;\n",
      " Minibatch[5301-5400]: loss = 163.933048 * 100, metric = 125.57% * 100;\n",
      " Minibatch[5401-5500]: loss = 163.932459 * 100, metric = 125.84% * 100;\n",
      " Minibatch[5501-5600]: loss = 163.931888 * 100, metric = 126.09% * 100;\n",
      " Minibatch[5601-5700]: loss = 163.931334 * 100, metric = 126.35% * 100;\n",
      " Minibatch[5701-5800]: loss = 163.930796 * 100, metric = 126.60% * 100;\n",
      " Minibatch[5801-5900]: loss = 163.930274 * 100, metric = 126.84% * 100;\n",
      " Minibatch[5901-6000]: loss = 163.929766 * 100, metric = 127.09% * 100;\n",
      " Minibatch[6001-6100]: loss = 163.929272 * 100, metric = 127.33% * 100;\n",
      " Minibatch[6101-6200]: loss = 163.928791 * 100, metric = 127.56% * 100;\n",
      " Minibatch[6201-6300]: loss = 163.928324 * 100, metric = 127.79% * 100;\n",
      " Minibatch[6301-6400]: loss = 163.927869 * 100, metric = 128.02% * 100;\n",
      " Minibatch[6401-6500]: loss = 163.927426 * 100, metric = 128.25% * 100;\n",
      " Minibatch[6501-6600]: loss = 163.926994 * 100, metric = 128.47% * 100;\n",
      " Minibatch[6601-6700]: loss = 163.926574 * 100, metric = 128.69% * 100;\n",
      " Minibatch[6701-6800]: loss = 163.926163 * 100, metric = 128.91% * 100;\n",
      " Minibatch[6801-6900]: loss = 163.925764 * 100, metric = 129.13% * 100;\n",
      " Minibatch[6901-7000]: loss = 163.925373 * 100, metric = 129.34% * 100;\n",
      " Minibatch[7001-7100]: loss = 163.924993 * 100, metric = 129.55% * 100;\n",
      " Minibatch[7101-7200]: loss = 163.924621 * 100, metric = 129.75% * 100;\n",
      " Minibatch[7201-7300]: loss = 163.924258 * 100, metric = 129.96% * 100;\n",
      " Minibatch[7301-7400]: loss = 163.923904 * 100, metric = 130.16% * 100;\n",
      " Minibatch[7401-7500]: loss = 163.923558 * 100, metric = 130.36% * 100;\n",
      " Minibatch[7501-7600]: loss = 163.923220 * 100, metric = 130.56% * 100;\n",
      " Minibatch[7601-7700]: loss = 163.922889 * 100, metric = 130.75% * 100;\n",
      " Minibatch[7701-7800]: loss = 163.922565 * 100, metric = 130.94% * 100;\n",
      " Minibatch[7801-7900]: loss = 163.922249 * 100, metric = 131.13% * 100;\n",
      " Minibatch[7901-8000]: loss = 163.921940 * 100, metric = 131.32% * 100;\n",
      " Minibatch[8001-8100]: loss = 163.921637 * 100, metric = 131.51% * 100;\n",
      " Minibatch[8101-8200]: loss = 163.921341 * 100, metric = 131.69% * 100;\n",
      " Minibatch[8201-8300]: loss = 163.921050 * 100, metric = 131.87% * 100;\n",
      " Minibatch[8301-8400]: loss = 163.920766 * 100, metric = 132.05% * 100;\n",
      " Minibatch[8401-8500]: loss = 163.920488 * 100, metric = 132.23% * 100;\n",
      " Minibatch[8501-8600]: loss = 163.920215 * 100, metric = 132.41% * 100;\n",
      " Minibatch[8601-8700]: loss = 163.919948 * 100, metric = 132.58% * 100;\n",
      " Minibatch[8701-8800]: loss = 163.919686 * 100, metric = 132.76% * 100;\n",
      " Minibatch[8801-8900]: loss = 163.919430 * 100, metric = 132.93% * 100;\n",
      " Minibatch[8901-9000]: loss = 163.919178 * 100, metric = 133.10% * 100;\n",
      " Minibatch[9001-9100]: loss = 163.918931 * 100, metric = 133.26% * 100;\n",
      " Minibatch[9101-9200]: loss = 163.918689 * 100, metric = 133.43% * 100;\n",
      " Minibatch[9201-9300]: loss = 163.918451 * 100, metric = 133.59% * 100;\n",
      " Minibatch[9301-9400]: loss = 163.918218 * 100, metric = 133.76% * 100;\n",
      " Minibatch[9401-9500]: loss = 163.917989 * 100, metric = 133.92% * 100;\n",
      " Minibatch[9501-9600]: loss = 163.917765 * 100, metric = 134.08% * 100;\n",
      " Minibatch[9601-9700]: loss = 163.917544 * 100, metric = 134.24% * 100;\n",
      " Minibatch[9701-9800]: loss = 163.917328 * 100, metric = 134.39% * 100;\n",
      " Minibatch[9801-9900]: loss = 163.917115 * 100, metric = 134.55% * 100;\n",
      " Minibatch[9901-10000]: loss = 163.916906 * 100, metric = 134.70% * 100;\n"
     ]
    }
   ],
   "source": [
    "dummy = c.input_variable(1, dtype=dtype)\n",
    "#criterion = 1.0 - totalProbability + dummy\n",
    "#criterion = 1.0 - occurrenceProbablilities + dummy\n",
    "criterion = 1.0 - logTotalProbability + dummy\n",
    "\n",
    "metric = c.reduce_mean(c.abs(c.reshape(deltaMeans, numberOfQueries) - realDeltaMeans))\n",
    "\n",
    "criterion = criterion\n",
    "lr_schedule = c.learning_rate_schedule(0.02, c.UnitType.minibatch)\n",
    "learner = c.sgd(totalProbability.parameters, lr=lr_schedule)\n",
    "\n",
    "progress_printer = c.logging.progress_print.ProgressPrinter(freq=100)\n",
    "trainer = c.Trainer(criterion, (criterion, metric), [learner], [progress_printer])\n",
    "\n",
    "\n",
    "#print((0+deltaMeans).eval())\n",
    "#print(intervalProbabilities.eval())\n",
    "#print(countsForIntervals.asarray())\n",
    "#print(occurrenceProbablilities.eval())\n",
    "#print(logTotalProbability.eval())\n",
    "\n",
    "for i in range(0, 10000):\n",
    "    trainer.train_minibatch({ dummy : np.asarray([0.0], dtype=np.float32) })\n",
    "    if i % 100 == -1:\n",
    "        #print((0+deltaMeans).eval())\n",
    "        #print(occurrenceProbablilities.eval())\n",
    "        print(intervalProbabilities.eval())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 616,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{Parameter('deltaStDevs', [], [10 x 1]): array([[ -1.88951346e-04],\n",
      "       [  4.66225008e-01],\n",
      "       [  3.98200112e-01],\n",
      "       [  1.91787472e-01],\n",
      "       [  3.48015274e-01],\n",
      "       [ -1.96455531e-04],\n",
      "       [ -1.96348092e-04],\n",
      "       [  3.63947376e-01],\n",
      "       [ -2.15499869e-02],\n",
      "       [ -2.53123782e-02]]), Parameter('deltaMeans', [], [10 x 1]): array([[  1.69928254e-05],\n",
      "       [  4.47572193e-01],\n",
      "       [  7.48336103e-01],\n",
      "       [  7.79195311e-02],\n",
      "       [ -1.73057320e-01],\n",
      "       [ -1.90492883e-05],\n",
      "       [  1.78409422e-05],\n",
      "       [  2.36851712e-01],\n",
      "       [ -8.88517874e-04],\n",
      "       [  7.40245359e-02]]), Parameter('innerBoundaries', [], [2]): array([-1.60838051,  0.19760649])}\n"
     ]
    }
   ],
   "source": [
    "df, fv = logTotalProbability.forward({}, [logTotalProbability.output], set([logTotalProbability.output]))\n",
    "#print(df, fv)\n",
    "value = list(fv.values())[0]\n",
    "#print(value)\n",
    "grad = logTotalProbability.backward(df, {logTotalProbability.output: np.ones_like(value)}, logTotalProbability.parameters, deltaStDevs)\n",
    "\n",
    "print(grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 617,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "innerBoundaries=\n",
      "[-2.28512551  3.25542094]\n",
      "realDeltaMeans=\n",
      "[ 4.87303609 -1.83526924 -1.58451526 -3.21890587  2.59622289 -6.90461609\n",
      "  5.23443529 -2.2836207   0.95711729 -0.74811113]\n",
      "deltaMeans=\n",
      "[[ 4.28245699]\n",
      " [-3.32880854]\n",
      " [-2.81699482]\n",
      " [-4.31803196]\n",
      " [ 4.34553693]\n",
      " [-5.36251144]\n",
      " [ 4.4167731 ]\n",
      " [-3.79286594]\n",
      " [ 3.25690975]\n",
      " [-1.90972831]]\n",
      "countsForIntervalsInit=\n",
      "[[ 0  0 50]\n",
      " [42  8  0]\n",
      " [35 15  0]\n",
      " [49  1  0]\n",
      " [ 0  2 48]\n",
      " [50  0  0]\n",
      " [ 0  0 50]\n",
      " [46  4  0]\n",
      " [ 2 23 25]\n",
      " [22 26  2]]\n",
      "[-0.5905791  -1.4935393  -1.23247956 -1.0991261   1.74931405  1.54210465\n",
      " -0.8176622  -1.50924524  2.29979247 -1.16161718]\n"
     ]
    }
   ],
   "source": [
    "print('innerBoundaries=')\n",
    "print(innerBoundaries.value)\n",
    "\n",
    "print('realDeltaMeans=')\n",
    "print(realDeltaMeans)\n",
    "\n",
    "print('deltaMeans=')\n",
    "print(deltaMeans.value)\n",
    "\n",
    "print('countsForIntervalsInit=')\n",
    "print(countsForIntervalsInit)\n",
    "\n",
    "print((c.reshape(deltaMeans, numberOfQueries) - realDeltaMeans).eval())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
